# -*- coding: utf-8 -*-
"""improved_arabic_law_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1puoArtUStqqdlcnEdRnw-uXjGdW-l6Wa
"""

# -*- coding: utf-8 -*-
"""improved_arabic_law_rag.py

Enhanced RAG system for Arabic legal texts with improved accuracy
"""

!pip install faiss-cpu sentence-transformers transformers nltk arabic-reshaper python-bidi
!python -m nltk.downloader stopwords punkt

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, util
import faiss
import re
import json
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
import arabic_reshaper
from bidi.algorithm import get_display
from datetime import datetime
from google.colab import files
import logging
import torch
from tqdm.notebook import tqdm

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('punkt')

class EnhancedArabicLawRAGSystem:
    def __init__(self, book_path):
        self.book_path = book_path
        self.paragraphs = []
        self.clean_paragraphs = []
        self.paragraph_metadata = []  # Store article numbers and section info
        self.generated_files = []
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {self.device}")
        self.initialize_components()
        self.prepare_book()

    def initialize_components(self):
        logger.info("Initializing NLP components...")

        # Enhanced stopwords with legal-specific terms to filter out
        self.stop_words = set(stopwords.words('arabic'))
        additional_stopwords = {'المادة', 'الفقرة', 'البند', 'القانون', 'العقوبات', 'يعاقب'}
        self.stop_words.update(additional_stopwords)

        # Use a more powerful multilingual embedding model
        logger.info("Loading embedding model...")
        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')
        self.embedding_model.to(self.device)

        # Use a specialized Arabic QA model
        logger.info("Loading QA model...")
        model_name = "ZeyadAhmed/AraElectra-Arabic-SQuADv2-QA"
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.qa_model = AutoModelForQuestionAnswering.from_pretrained(model_name)
        self.qa_model.to(self.device)

        # Query expansion model
        logger.info("Loading cross-encoder for reranking...")

        # In initialize_components() method:
        self.cross_encoder = pipeline(
            "text-classification",
            model="cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
            device=0 if torch.cuda.is_available() else -1,
            padding=True,     # Add padding
            truncation=True   # Add truncation
        )

        # Reset result storage
        self.search_results = []
        self.qa_results = []

    def clean_arabic_text(self, text):
        """Enhanced cleaning function for Arabic text"""
        # Remove diacritics (tashkeel)
        text = re.sub(r'[\u064B-\u065F]', '', text)

        # Normalize various forms of Alef and Ya
        text = re.sub(r'[إأآا]', 'ا', text)
        text = re.sub(r'[ىي]', 'ي', text)
        text = re.sub(r'ة', 'ه', text)

        # Remove punctuation but preserve sentence structure
        text = re.sub(r'[^\w\s\u0600-\u06FF.!؟]', ' ', text)

        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()

        # Remove stopwords
        text = ' '.join([word for word in text.split() if word not in self.stop_words])

        return text

    def extract_article_number(self, text):
        """Extract article numbers from text using regex patterns"""
        # Match patterns like "المادة 123" or "مادة 123"
        match = re.search(r'(?:المادة|مادة)\s+(\d+)', text)
        if match:
            return f"المادة {match.group(1)}"
        return None

    def extract_legal_sections(self, text):
        """Extract legal sections based on semantic content"""
        crime_keywords = {'جناية', 'جنحة', 'مخالفة', 'جريمة', 'عقوبة', 'سجن', 'حبس', 'غرامة'}
        punishment_keywords = {'يعاقب', 'العقوبة', 'السجن', 'الحبس', 'الغرامة', 'الإعدام', 'أشغال شاقة'}

        sections = []
        if any(keyword in text for keyword in crime_keywords):
            sections.append('crime_definition')
        if any(keyword in text for keyword in punishment_keywords):
            sections.append('punishment')

        return sections if sections else ['general']

    def split_into_meaningful_chunks(self, text):
        """Split text into meaningful chunks based on articles and logical sections"""
        # First split by article patterns
        article_pattern = r'(?:المادة|مادة)\s+\d+'
        article_splits = re.split(f'({article_pattern})', text)

        paragraphs = []
        metadata = []

        current_article = None

        for i in range(0, len(article_splits), 2):
            if i+1 < len(article_splits):
                article_header = article_splits[i+1]
                article_content = article_splits[i+2] if i+2 < len(article_splits) else ""

                # Extract article number
                current_article = self.extract_article_number(article_header)

                # Split article content into logical paragraphs
                content_splits = re.split(r'(?<=[.!؟])\s+', article_content)

                current_paragraph = []
                for sentence in content_splits:
                    if len(sentence.strip()) > 10:  # Ensure it's a meaningful sentence
                        current_paragraph.append(sentence)

                        # Create chunks of 2-3 sentences for better context preservation
                        if len(current_paragraph) >= 2:
                            paragraph_text = ' '.join(current_paragraph)
                            sections = self.extract_legal_sections(paragraph_text)

                            paragraphs.append(paragraph_text)
                            metadata.append({
                                'article': current_article,
                                'sections': sections,
                                'char_length': len(paragraph_text)
                            })

                            # Keep the last sentence for context continuity
                            current_paragraph = [current_paragraph[-1]]

            # Handle text before the first article if it exists
            elif i == 0 and article_splits[i].strip():
                intro_text = article_splits[i]
                sentences = re.split(r'(?<=[.!؟])\s+', intro_text)

                current_paragraph = []
                for sentence in sentences:
                    if len(sentence.strip()) > 10:
                        current_paragraph.append(sentence)
                        if len(current_paragraph) >= 2:
                            paragraph_text = ' '.join(current_paragraph)
                            sections = self.extract_legal_sections(paragraph_text)

                            paragraphs.append(paragraph_text)
                            metadata.append({
                                'article': 'مقدمة',
                                'sections': sections,
                                'char_length': len(paragraph_text)
                            })

                            current_paragraph = [current_paragraph[-1]]

        return paragraphs, metadata

    def prepare_book(self):
        logger.info(f"Reading text from {self.book_path}...")
        with open(self.book_path, 'r', encoding='utf-8') as f:
            text = f.read()

        logger.info("Splitting text into meaningful chunks...")
        self.paragraphs, self.paragraph_metadata = self.split_into_meaningful_chunks(text)

        if not self.paragraphs:
            raise ValueError("No valid paragraphs found in the text")

        logger.info(f"Successfully extracted {len(self.paragraphs)} paragraphs with metadata")

        # Clean paragraphs for indexing
        logger.info("Cleaning paragraphs for indexing...")
        self.clean_paragraphs = [self.clean_arabic_text(p) for p in self.paragraphs]

        # Build traditional TF-IDF index
        logger.info("Building TF-IDF index...")
        self.tfidf_vectorizer = TfidfVectorizer(
            ngram_range=(1, 2),  # Include bigrams
            max_df=0.85,         # Filter out terms that appear in >85% of documents
            min_df=2             # Filter out terms that appear in <2 documents
        )
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.clean_paragraphs)

        # Build dense vector index for semantic search
        logger.info("Building dense vector index...")
        embeddings = self.embedding_model.encode(
            self.paragraphs,
            convert_to_tensor=True,
            show_progress_bar=True
        )
        embeddings = embeddings.cpu().numpy()

        # Use IVF index for faster search with minimal accuracy loss
        logger.info("Building FAISS index...")
        embedding_dim = embeddings.shape[1]

        # Use L2 normalization for cosine similarity
        faiss.normalize_L2(embeddings)

        if len(self.paragraphs) > 1000:
            # For larger collections, use IVF index
            n_clusters = min(int(np.sqrt(len(self.paragraphs))), 100)
            quantizer = faiss.IndexFlatIP(embedding_dim)
            self.faiss_index = faiss.IndexIVFFlat(quantizer, embedding_dim, n_clusters, faiss.METRIC_INNER_PRODUCT)
            self.faiss_index.train(embeddings)
        else:
            # For smaller collections, use flat index
            self.faiss_index = faiss.IndexFlatIP(embedding_dim)

        self.faiss_index.add(embeddings.astype(np.float32))
        logger.info("Book preparation complete")

    def expand_query(self, query):
        """Expand query with legal terminology based on the query content"""
        crime_terms = {
            'سرقة': ['سرق', 'سارق', 'المسروق', 'المسروقات'],
            'قتل': ['قاتل', 'مقتول', 'القتل العمد', 'القتل الخطأ'],
            'رشوة': ['رشا', 'مرتشي', 'راشي', 'الرشوة'],
            'تزوير': ['زور', 'مزور', 'تزييف', 'وثائق مزورة'],
            'اختلاس': ['اختلس', 'مختلس', 'الاختلاس', 'اموال عامة'],
            'مخدرات': ['تجارة المخدرات', 'الاتجار', 'حيازة المخدرات', 'تعاطي'],
            'عنف': ['عنف أسري', 'اعتداء', 'ضرب', 'إيذاء'],
            'تحرش': ['تحرش جنسي', 'التحرش', 'المتحرش', 'المجني عليه'],
            'إلكترونية': ['جرائم إلكترونية', 'جرائم الإنترنت', 'الاحتيال الإلكتروني'],
            'هروب': ['هروب من السجن', 'الهارب', 'الفرار']
        }

        expanded_terms = []
        clean_query = self.clean_arabic_text(query)

        for term, expansions in crime_terms.items():
            if term in clean_query:
                expanded_terms.extend(expansions)

        if expanded_terms:
            expanded_query = f"{query} {' '.join(expanded_terms)}"
            logger.info(f"Expanded query: {query} -> {expanded_query}")
            return expanded_query

        return query

    def classical_search(self, query, top_k=10):
        """Enhanced TF-IDF based search with legal term weighting"""
        expanded_query = self.expand_query(query)
        clean_query = self.clean_arabic_text(expanded_query)
        query_vec = self.tfidf_vectorizer.transform([clean_query])

        # Get similarity scores
        sim_scores = cosine_similarity(query_vec, self.tfidf_matrix).flatten()

        # Boost scores for paragraphs with relevant legal sections
        query_keywords = set(clean_query.split())
        crime_keywords = {'جناية', 'جنحة', 'مخالفة', 'جريمة', 'عقوبة', 'سجن', 'حبس', 'غرامة'}
        is_crime_query = bool(query_keywords.intersection(crime_keywords))

        # Apply score boosting based on metadata
        boosted_scores = sim_scores.copy()
        for i, metadata in enumerate(self.paragraph_metadata):
            # Boost paragraphs from relevant sections
            if is_crime_query and 'punishment' in metadata['sections']:
                boosted_scores[i] *= 1.3
            elif 'crime_definition' in metadata['sections']:
                boosted_scores[i] *= 1.2

        # Return top results
        top_indices = np.argsort(boosted_scores)[-top_k:][::-1]
        return [(self.paragraphs[i], float(boosted_scores[i]), self.paragraph_metadata[i]) for i in top_indices]

    def semantic_search(self, query, top_k=10):
        """Enhanced semantic search with query expansion"""
        expanded_query = self.expand_query(query)
        query_embedding = self.embedding_model.encode([expanded_query])

        # Normalize for cosine similarity
        faiss.normalize_L2(query_embedding.astype(np.float32))

        # Search FAISS index
        distances, indices = self.faiss_index.search(query_embedding.astype(np.float32), min(top_k*2, len(self.paragraphs)))

        results = [(self.paragraphs[i], float(distances[0][j]), self.paragraph_metadata[i])
                  for j, i in enumerate(indices[0])
                  if i < len(self.paragraphs)]

        # Rerank top results with cross-encoder if available
        if hasattr(self, 'cross_encoder') and results:
            try:
                pairs = [(expanded_query, result[0]) for result in results]
                rerank_scores = self.cross_encoder(pairs, function_to_apply="softmax", top_k=None)

                # Extract scores from the positive class
                scores = [score[0]['score'] for score in rerank_scores]

                # Rerank results based on cross-encoder scores
                reranked_results = [(results[i][0], scores[i], results[i][2]) for i in range(len(results))]
                reranked_results.sort(key=lambda x: x[1], reverse=True)

                # Take top-k
                return reranked_results[:top_k]
            except Exception as e:
                logger.warning(f"Cross-encoder reranking failed: {e}")

        # If reranking fails or cross-encoder not available, return original results
        return results[:top_k]

    def hybrid_search(self, query, top_k=7):
        """Hybrid search combining classical and semantic search"""
        # Get results from both methods
        classical_results = self.classical_search(query, top_k=top_k)
        semantic_results = self.semantic_search(query, top_k=top_k)

        # Combine results with proper weighting
        combined_results = {}

        # Add classical results
        for text, score, metadata in classical_results:
            combined_results[text] = {
                'score': score * 0.4,  # Weight for classical search
                'metadata': metadata
            }

        # Add semantic results with higher weight
        for text, score, metadata in semantic_results:
            if text in combined_results:
                combined_results[text]['score'] += score * 0.6  # Weight for semantic search
            else:
                combined_results[text] = {
                    'score': score * 0.6,  # Weight for semantic search
                    'metadata': metadata
                }

        # Sort by combined score
        sorted_results = [(text, data['score'], data['metadata'])
                          for text, data in sorted(combined_results.items(),
                                                 key=lambda x: x[1]['score'],
                                                 reverse=True)]

        return sorted_results[:top_k]

    def get_enhanced_context(self, question, top_k=5):
        """Build enhanced context for QA model with better filtering and ordering"""
        try:
            # Use hybrid search
            search_results = self.hybrid_search(question, top_k=top_k)

            if not search_results:
                return "لا يوجد معلومات ذات صلة"

            # Filter results with threshold
            filtered_results = [result for result in search_results if result[1] > 0.2]

            if not filtered_results:
                # Fall back to top 2 results if all are below threshold
                filtered_results = search_results[:2]

            # Build context with article numbers and proper ordering
            context_paragraphs = []
            for text, score, metadata in filtered_results:
                article_prefix = f"{metadata['article']}: " if metadata['article'] else ""
                context_paragraphs.append(f"{article_prefix}{text}")

            # Join paragraphs with proper separator for context
            return "\n\n".join(context_paragraphs)

        except Exception as e:
            logger.error(f"Error retrieving context: {e}")
            return "لا يوجد معلومات ذات صلة - خطأ في استرجاع المعلومات"

    def answer_question_improved(self, question):
        """Enhanced QA with better context handling and confidence scoring"""
        # Get enhanced context
        context = self.get_enhanced_context(question)

        # Prepare result object
        result = {
            'question': question,
            'rag_answer': "لا يوجد جواب",
            'rag_confidence': 0.0,
            'llm_answer': "لا يوجد جواب",
            'context_used': context.split('\n\n') if context else []
        }

        # Calculate maximum input length based on model's token limit
        max_token_length = self.tokenizer.model_max_length

        try:
            # Process QA with context (RAG approach)
            if context and context != "لا يوجد معلومات ذات صلة":
                # Check if context fits in token limit
                inputs = self.tokenizer(question, context, return_tensors="pt", truncation=True, max_length=max_token_length)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

                with torch.no_grad():
                    outputs = self.qa_model(**inputs)

                # Get answer
                answer_start = torch.argmax(outputs.start_logits)
                answer_end = torch.argmax(outputs.end_logits)
                answer_confidence = float(torch.max(outputs.start_logits).item() + torch.max(outputs.end_logits).item()) / 2

                # Convert to normalized confidence score (0-1)
                answer_confidence = min(max(answer_confidence / 10, 0), 1)

                # Ensure start <= end and within bounds
                if answer_start <= answer_end and answer_end < inputs["input_ids"].shape[1]:
                    answer = self.tokenizer.decode(inputs["input_ids"][0][answer_start:answer_end+1])
                    if answer and len(answer.strip()) > 0:
                        result['rag_answer'] = answer
                        result['rag_confidence'] = answer_confidence

                # If answer is too short or empty, try retrieval for different spans
                if len(result['rag_answer'].strip()) < 5:
                    # Get top 3 start and end positions
                    start_scores = torch.topk(outputs.start_logits, k=3)
                    end_scores = torch.topk(outputs.end_logits, k=3)

                    for i in range(3):
                        for j in range(3):
                            s_idx = start_scores.indices[0][i].item()
                            e_idx = end_scores.indices[0][j].item()

                            if s_idx <= e_idx and e_idx < inputs["input_ids"].shape[1] and e_idx - s_idx < 30:
                                candidate = self.tokenizer.decode(inputs["input_ids"][0][s_idx:e_idx+1])
                                if len(candidate.strip()) > len(result['rag_answer'].strip()):
                                    result['rag_answer'] = candidate
                                    result['rag_confidence'] = float(start_scores.values[0][i].item() +
                                                                   end_scores.values[0][j].item()) / 20

            # Process QA without context (LLM-only approach)
            inputs = self.tokenizer(question, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.qa_model(**inputs)

            # Get LLM-only answer
            answer_start = torch.argmax(outputs.start_logits)
            answer_end = torch.argmax(outputs.end_logits)

            if answer_start <= answer_end and answer_end < inputs["input_ids"].shape[1]:
                answer = self.tokenizer.decode(inputs["input_ids"][0][answer_start:answer_end+1])
                if answer and len(answer.strip()) > 0:
                    result['llm_answer'] = answer

            # Post-process answers
            if result['rag_answer'] == "لا يوجد جواب" and result['rag_confidence'] < 0.2:
                result['rag_answer'] = "لا يمكن الإجابة بناء على المعلومات المتاحة"

            if result['llm_answer'] == "لا يوجد جواب":
                result['llm_answer'] = "لا يمكن الإجابة بدون معلومات مرجعية"

        except Exception as e:
            logger.error(f"Error in QA processing: {e}")
            result['rag_answer'] = "حدث خطأ أثناء معالجة السؤال"
            result['llm_answer'] = "حدث خطأ أثناء معالجة السؤال"

        return result

    def run_benchmark(self, queries):
        """Run benchmark on a list of queries with progress tracking"""
        logger.info(f"Running benchmark on {len(queries)} queries...")

        for i, query in enumerate(tqdm(queries, desc="Processing Queries")):
            # Run hybrid search
            hybrid_results = self.hybrid_search(query)

            # Also get classical and semantic results for comparison
            classical = self.classical_search(query)
            semantic = self.semantic_search(query)

            # Store search results
            self.search_results.append({
                'query': query,
                'hybrid': [p[0] for p in hybrid_results],
                'classical': [p[0] for p in classical],
                'semantic': [p[0] for p in semantic],
                'hybrid_scores': [p[1] for p in hybrid_results],
                'classical_scores': [p[1] for p in classical],
                'semantic_scores': [p[1] for p in semantic],
                'hybrid_articles': [p[2]['article'] if 'article' in p[2] else None for p in hybrid_results]
            })

            # Run improved QA
            qa_result = self.answer_question_improved(query)
            self.qa_results.append(qa_result)

            logger.info(f"Query {i+1}/{len(queries)} processed")

    def evaluate_search_quality(self):
        """Evaluate search quality metrics"""
        evaluation = {
            'avg_hybrid_score': np.mean([result['hybrid_scores'][0] if result['hybrid_scores'] else 0
                                         for result in self.search_results]),
            'avg_semantic_score': np.mean([result['semantic_scores'][0] if result['semantic_scores'] else 0
                                          for result in self.search_results]),
            'avg_classical_score': np.mean([result['classical_scores'][0] if result['classical_scores'] else 0
                                           for result in self.search_results]),
            'hybrid_above_threshold': sum(1 for result in self.search_results
                                         if result['hybrid_scores'] and result['hybrid_scores'][0] > 0.4),
            'semantic_above_threshold': sum(1 for result in self.search_results
                                           if result['semantic_scores'] and result['semantic_scores'][0] > 0.4),
            'classical_above_threshold': sum(1 for result in self.search_results
                                            if result['classical_scores'] and result['classical_scores'][0] > 0.4)
        }
        return evaluation

    def evaluate_qa_quality(self):
        """Evaluate QA quality metrics"""
        evaluation = {
            'avg_rag_confidence': np.mean([result['rag_confidence'] for result in self.qa_results]),
            'high_confidence_answers': sum(1 for result in self.qa_results if result['rag_confidence'] > 0.6),
            'medium_confidence_answers': sum(1 for result in self.qa_results if 0.3 < result['rag_confidence'] <= 0.6),
            'low_confidence_answers': sum(1 for result in self.qa_results if result['rag_confidence'] <= 0.3),
            'avg_answer_length': np.mean([len(result['rag_answer']) for result in self.qa_results])
        }
        return evaluation

    def generate_reports(self):
        """Generate comprehensive evaluation reports"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.generated_files = []

        # Book analysis report
        book_file = f'book_preparation_{timestamp}.json'
        with open(book_file, 'w', encoding='utf-8') as f:
            json.dump({
                'book_title': 'قانون العقوبات المصرى',
                'source_file': self.book_path,
                'total_paragraphs': len(self.paragraphs),
                'paragraph_stats': {
                    'avg_length': np.mean([len(p) for p in self.paragraphs]),
                    'max_length': max([len(p) for p in self.paragraphs]),
                    'min_length': min([len(p) for p in self.paragraphs])
                },
                'article_count': len(set(m['article'] for m in self.paragraph_metadata if m['article'])),
                'chunking_method': 'تقسيم ذكي بناءً على المواد والسياق القانوني',
                'embedding_model': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
                'sample_paragraphs': self.paragraphs[:3],
                'sample_metadata': self.paragraph_metadata[:3]
            }, f, ensure_ascii=False, indent=2)
        self.generated_files.append(book_file)

        # Search results detail report
        search_file = f'search_results_{timestamp}.csv'
        search_df = pd.DataFrame(self.search_results)
        search_df.to_csv(search_file, index=False, encoding='utf-8-sig')
        self.generated_files.append(search_file)

        # QA results detail report
        qa_file = f'qa_results_{timestamp}.csv'
        qa_df = pd.DataFrame([{
            'question': r['question'],
            'rag_answer': r['rag_answer'],
            'rag_confidence': r['rag_confidence'],
            'llm_answer': r['llm_answer'],
            'context_paragraphs': len(r['context_used']),
            'context_sample': r['context_used'][0] if r['context_used'] else "لا يوجد"
        } for r in self.qa_results])
        qa_df.to_csv(qa_file, index=False, encoding='utf-8-sig')
        self.generated_files.append(qa_file)

        # Comprehensive analysis report
        comparison_file = f'enhanced_analysis_report_{timestamp}.txt'
        with open(comparison_file, 'w', encoding='utf-8') as f:
            f.write(self.generate_comprehensive_analysis())
        self.generated_files.append(comparison_file)

        # Performance visualization (HTML)
        visualization_file = f'performance_visualization_{timestamp}.html'
        with open(visualization_file, 'w', encoding='utf-8') as f:
            f.write(self.generate_visualization_html())
        self.generated_files.append(visualization_file)

        logger.info(f"Generated {len(self.generated_files)} reports")

    def generate_comprehensive_analysis(self):
        """Generate a comprehensive analysis of system performance"""
        search_eval = self.evaluate_search_quality()
        qa_eval = self.evaluate_qa_quality()

        analysis = "# تقرير التحليل الشامل لنظام استرجاع المعلومات القانونية\n\n"

        # Add system overview
        analysis += "## نظرة عامة على النظام\n"
        analysis += "- تم تحليل قانون العقوبات المصري وتقسيمه إلى وحدات نصية ذات معنى\n"
        analysis += f"- إجمالي عدد الفقرات المستخرجة: {len(self.paragraphs)}\n"
        analysis += f"- عدد المواد القانونية المميزة: {len(set(m['article'] for m in self.paragraph_metadata if m['article']))}\n\n"

        # Add search performance analysis
        analysis += "## أداء البحث المعلوماتي\n"
        analysis += f"- متوسط دقة البحث الهجين: {search_eval['avg_hybrid_score']:.4f}\n"
        analysis += f"- متوسط دقة البحث الدلالي: {search_eval['avg_semantic_score']:.4f}\n"
        analysis += f"- متوسط دقة البحث التقليدي: {search_eval['avg_classical_score']:.4f}\n"
        analysis += f"- نتائج البحث الهجين فوق العتبة: {search_eval['hybrid_above_threshold']}/{len(self.search_results)}\n"
        analysis += f"- نتائج البحث الدلالي فوق العتبة: {search_eval['semantic_above_threshold']}/{len(self.search_results)}\n"
        analysis += f"- نتائج البحث التقليدي فوق العتبة: {search_eval['classical_above_threshold']}/{len(self.search_results)}\n\n"

        # Add QA performance analysis
        analysis += "## أداء نظام الإجابة على الأسئلة\n"
        analysis += f"- متوسط ثقة الإجابات المستندة إلى RAG: {qa_eval['avg_rag_confidence']:.4f}\n"
        analysis += f"- عدد الإجابات عالية الثقة: {qa_eval['high_confidence_answers']}/{len(self.qa_results)}\n"
        analysis += f"- عدد الإجابات متوسطة الثقة: {qa_eval['medium_confidence_answers']}/{len(self.qa_results)}\n"
        analysis += f"- عدد الإجابات منخفضة الثقة: {qa_eval['low_confidence_answers']}/{len(self.qa_results)}\n"
        analysis += f"- متوسط طول الإجابات: {qa_eval['avg_answer_length']:.1f} حرف\n\n"

        # Add sample of best performing queries
        analysis += "## أمثلة على الأسئلة الأفضل أداءً\n"
        best_performing = sorted(self.qa_results, key=lambda x: x['rag_confidence'], reverse=True)[:3]
        for i, result in enumerate(best_performing):
            analysis += f"### سؤال {i+1}: {result['question']}\n"
            analysis += f"- الإجابة: {result['rag_answer']}\n"
            analysis += f"- مستوى الثقة: {result['rag_confidence']:.4f}\n"
            if result['context_used']:
                analysis += f"- مصدر المعلومات: {result['context_used'][0][:100]}...\n\n"

        # Add sample of worst performing queries
        analysis += "## أمثلة على الأسئلة الأقل أداءً\n"
        worst_performing = sorted(self.qa_results, key=lambda x: x['rag_confidence'])[:3]
        for i, result in enumerate(worst_performing):
            analysis += f"### سؤال {i+1}: {result['question']}\n"
            analysis += f"- الإجابة: {result['rag_answer']}\n"
            analysis += f"- مستوى الثقة: {result['rag_confidence']:.4f}\n\n"

        # Add comparison between RAG and LLM-only answers
        analysis += "## مقارنة بين إجابات RAG والإجابات بدون سياق\n"
        better_with_rag = sum(1 for r in self.qa_results
                             if len(r['rag_answer']) > len(r['llm_answer']) * 1.2 and r['rag_confidence'] > 0.4)
        analysis += f"- عدد الأسئلة التي تحسنت بشكل ملحوظ باستخدام RAG: {better_with_rag}/{len(self.qa_results)}\n\n"

        # Add recommendations
        analysis += "## التوصيات للتحسين\n"
        analysis += "1. تحسين تقسيم النص وفقًا للمواد القانونية بشكل أكثر دقة\n"
        analysis += "2. استخدام نماذج متخصصة في اللغة العربية القانونية\n"
        analysis += "3. إضافة مصادر قانونية إضافية مثل قرارات المحاكم والتعليقات على القانون\n"
        analysis += "4. تحسين توسيع الاستعلام باستخدام مفاهيم قانونية أكثر تخصصًا\n"
        analysis += "5. تطبيق تقنيات متقدمة للتعامل مع المصطلحات القانونية المتخصصة\n"

        return analysis

    def generate_visualization_html(self):
        """Generate HTML visualization of performance metrics"""
        search_eval = self.evaluate_search_quality()
        qa_eval = self.evaluate_qa_quality()

        # Create simple HTML with bar charts using basic JS
        html = """
        <!DOCTYPE html>
        <html dir="rtl" lang="ar">
        <head>
            <meta charset="UTF-8">
            <title>تحليل أداء نظام استرجاع المعلومات القانونية</title>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.7.0/chart.min.js"></script>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; background-color: #f9f9f9; }
                .container { max-width: 1000px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }
                h1, h2 { color: #2c3e50; }
                .chart-container { position: relative; height: 300px; margin-bottom: 30px; }
                .metric-box { display: inline-block; background-color: #f1f8ff; border-radius: 5px; padding: 15px; margin: 10px; width: 200px; text-align: center; }
                .metric-value { font-size: 24px; font-weight: bold; color: #3498db; }
                .metric-label { font-size: 14px; color: #7f8c8d; }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>تحليل أداء نظام استرجاع المعلومات القانونية</h1>

                <h2>مقاييس أداء البحث</h2>
                <div class="metrics-container">
                    <div class="metric-box">
                        <div class="metric-value">""" + f"{search_eval['avg_hybrid_score']:.4f}" + """</div>
                        <div class="metric-label">متوسط دقة البحث الهجين</div>
                    </div>
                    <div class="metric-box">
                        <div class="metric-value">""" + f"{search_eval['hybrid_above_threshold']}" + """</div>
                        <div class="metric-label">نتائج فوق العتبة من """ + f"{len(self.search_results)}" + """</div>
                    </div>
                </div>

                <h2>مقارنة أداء أساليب البحث</h2>
                <div class="chart-container">
                    <canvas id="searchMethodsChart"></canvas>
                </div>

                <h2>مقاييس أداء الإجابة على الأسئلة</h2>
                <div class="metrics-container">
                    <div class="metric-box">
                        <div class="metric-value">""" + f"{qa_eval['avg_rag_confidence']:.4f}" + """</div>
                        <div class="metric-label">متوسط ثقة الإجابات</div>
                    </div>
                    <div class="metric-box">
                        <div class="metric-value">""" + f"{qa_eval['high_confidence_answers']}" + """</div>
                        <div class="metric-label">الإجابات عالية الثقة</div>
                    </div>
                </div>

                <h2>توزيع مستويات الثقة</h2>
                <div class="chart-container">
                    <canvas id="confidenceLevelsChart"></canvas>
                </div>
            </div>

            <script>
                // Search methods comparison chart
                var searchCtx = document.getElementById('searchMethodsChart').getContext('2d');
                var searchChart = new Chart(searchCtx, {
                    type: 'bar',
                    data: {
                        labels: ['البحث الهجين', 'البحث الدلالي', 'البحث التقليدي'],
                        datasets: [{
                            label: 'متوسط الدقة',
                            data: [""" + f"{search_eval['avg_hybrid_score']:.4f}, {search_eval['avg_semantic_score']:.4f}, {search_eval['avg_classical_score']:.4f}" + """],
                            backgroundColor: [
                                'rgba(54, 162, 235, 0.5)',
                                'rgba(75, 192, 192, 0.5)',
                                'rgba(255, 206, 86, 0.5)'
                            ],
                            borderColor: [
                                'rgba(54, 162, 235, 1)',
                                'rgba(75, 192, 192, 1)',
                                'rgba(255, 206, 86, 1)'
                            ],
                            borderWidth: 1
                        }, {
                            label: 'النتائج فوق العتبة',
                            data: [""" + f"{search_eval['hybrid_above_threshold']}, {search_eval['semantic_above_threshold']}, {search_eval['classical_above_threshold']}" + """],
                            backgroundColor: [
                                'rgba(54, 162, 235, 0.2)',
                                'rgba(75, 192, 192, 0.2)',
                                'rgba(255, 206, 86, 0.2)'
                            ],
                            borderColor: [
                                'rgba(54, 162, 235, 1)',
                                'rgba(75, 192, 192, 1)',
                                'rgba(255, 206, 86, 1)'
                            ],
                            borderWidth: 1
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: true
                            }
                        }
                    }
                });

                // Confidence levels chart
                var confCtx = document.getElementById('confidenceLevelsChart').getContext('2d');
                var confChart = new Chart(confCtx, {
                    type: 'pie',
                    data: {
                        labels: ['عالية الثقة', 'متوسطة الثقة', 'منخفضة الثقة'],
                        datasets: [{
                            data: [""" + f"{qa_eval['high_confidence_answers']}, {qa_eval['medium_confidence_answers']}, {qa_eval['low_confidence_answers']}" + """],
                            backgroundColor: [
                                'rgba(46, 204, 113, 0.7)',
                                'rgba(52, 152, 219, 0.7)',
                                'rgba(231, 76, 60, 0.7)'
                            ],
                            borderColor: [
                                'rgba(46, 204, 113, 1)',
                                'rgba(52, 152, 219, 1)',
                                'rgba(231, 76, 60, 1)'
                            ],
                            borderWidth: 1
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false
                    }
                });
            </script>
        </body>
        </html>
        """
        return html

    def download_reports(self):
        """Download all generated reports"""
        for fname in self.generated_files:
            try:
                files.download(fname)
                logger.info(f"Downloaded file: {fname}")
            except Exception as e:
                logger.error(f"Failed to download {fname}: {e}")

# Execution
if __name__ == "__main__":
    try:
        logger.info("Starting the enhanced Arabic Law RAG system...")

        # Upload the law document
        uploaded = files.upload()
        if not uploaded:
            raise ValueError("لم يتم تحميل أي ملف")

        file_path = list(uploaded.keys())[0]
        logger.info(f"Uploaded file: {file_path}")

        # Initialize and run the system
        system = EnhancedArabicLawRAGSystem(file_path)

        # Define benchmark queries with legal focus
        queries = [
            "ما هي عقوبة السرقة المشددة؟",
            "كيف يعرف القانون جريمة الرشوة؟",
            "ما عقوبة القتل الخطأ؟",
            "ما هي أحكام التزوير في المستندات الرسمية؟",
            "كيف يعاقب القانون على الاختلاس من المال العام؟",
            "ما هي عقوبة الاتجار بالمخدرات وفقاً للقانون؟",
            "كيف يعالج القانون العنف الأسري والاعتداء على الأسرة؟",
            "ما هي عقوبة التحرش الجنسي في قانون العقوبات؟",
            "كيف ينظم قانون العقوبات الجرائم الإلكترونية والاحتيال؟",
            "ما هي عقوبة الهروب من السجن والشروع فيه؟",
            "ما هي ظروف تشديد العقوبة في جرائم السرقة؟",
            "كيف يميز القانون بين أنواع القتل المختلفة؟",
            "ما هي أركان جريمة النصب والاحتيال؟",
            "كيف يعاقب القانون على إخفاء الأشياء المسروقة؟",
            "ما هي حالات الإعفاء من العقاب في قانون العقوبات؟"
        ]

        # Run the benchmark
        logger.info("Running benchmark...")
        system.run_benchmark(queries)

        # Generate and download reports
        logger.info("Generating reports...")
        system.generate_reports()
        system.download_reports()

        logger.info("تم الانتهاء من جميع المهام بنجاح!")

    except Exception as e:
        logger.error(f"حدث خطأ: {e}")
        import traceback
        logger.error(traceback.format_exc())